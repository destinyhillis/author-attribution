{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f97923",
   "metadata": {},
   "source": [
    "# Author Attribution Using Stylometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bb607",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "%pip install nltk\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81023366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b89cc",
   "metadata": {},
   "source": [
    "### Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9496c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094569e8",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb1072",
   "metadata": {},
   "source": [
    "### Get File IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a620ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gutenberg_fileids():\n",
    "    \"\"\"Get the list of file IDs from the Gutenberg corpus\"\"\"\n",
    "    file_ids = gutenberg.fileids()\n",
    "    \n",
    "    return file_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20aef2",
   "metadata": {},
   "source": [
    "#### Test get_gutenberg_fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242f4aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gutenberg_fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7402855",
   "metadata": {},
   "source": [
    "### Extract Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44c149f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(file_ids):\n",
    "    \"\"\"Extract author names from file IDs\"\"\"\n",
    "    authors = {}\n",
    "    for file_id in file_ids:\n",
    "        author = file_id.split('-')[0]\n",
    "        authors[file_id] = author\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b1292",
   "metadata": {},
   "source": [
    "#### Test extract_authors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab2c8224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen-emma.txt': 'austen',\n",
       " 'austen-persuasion.txt': 'austen',\n",
       " 'austen-sense.txt': 'austen',\n",
       " 'bible-kjv.txt': 'bible',\n",
       " 'blake-poems.txt': 'blake',\n",
       " 'bryant-stories.txt': 'bryant',\n",
       " 'burgess-busterbrown.txt': 'burgess',\n",
       " 'carroll-alice.txt': 'carroll',\n",
       " 'chesterton-ball.txt': 'chesterton',\n",
       " 'chesterton-brown.txt': 'chesterton',\n",
       " 'chesterton-thursday.txt': 'chesterton',\n",
       " 'edgeworth-parents.txt': 'edgeworth',\n",
       " 'melville-moby_dick.txt': 'melville',\n",
       " 'milton-paradise.txt': 'milton',\n",
       " 'shakespeare-caesar.txt': 'shakespeare',\n",
       " 'shakespeare-hamlet.txt': 'shakespeare',\n",
       " 'shakespeare-macbeth.txt': 'shakespeare',\n",
       " 'whitman-leaves.txt': 'whitman'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_authors(get_gutenberg_fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b004d7",
   "metadata": {},
   "source": [
    "### Get Text Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f98dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(file_ids, chunk_size=2000, overlap=0):\n",
    "    \"\"\"\n",
    "    Extract text samples from the corpus.\n",
    "    \n",
    "    Args:\n",
    "        chunk_size: Size of each text sample in words\n",
    "        overlap: Number of words to overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of (text, author) tuples\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for file_id in file_ids:\n",
    "        text = gutenberg.raw(file_id)\n",
    "        \n",
    "        # Clean text by removing metadata (usually at the beginning and end)\n",
    "        # This is a simple approach - might need refinement for specific corpora\n",
    "        lines = text.split('\\n')\n",
    "        start_idx = 0\n",
    "        end_idx = len(lines)\n",
    "        \n",
    "        # Find start of main text (skip headers)\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"*** START OF\" in line:\n",
    "                start_idx = i + 1\n",
    "                break\n",
    "                \n",
    "        # Find end of main text (skip footers)\n",
    "        for i, line in enumerate(lines[::-1]):\n",
    "            if \"*** END OF\" in line:\n",
    "                end_idx = len(lines) - i - 1\n",
    "                break\n",
    "                \n",
    "        clean_text = ' '.join(lines[start_idx:end_idx])\n",
    "        \n",
    "        # Split into chunks\n",
    "        words = word_tokenize(clean_text)\n",
    "        step = chunk_size - overlap\n",
    "        \n",
    "        for i in range(0, len(words) - chunk_size + 1, step):\n",
    "            chunk = ' '.join(words[i:i+chunk_size])\n",
    "            samples.append((chunk, file_id))\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87ca85",
   "metadata": {},
   "source": [
    "#### Test get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac122be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: [(\"[ Poems by William Blake 1789 ] SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping down the valleys wild , Piping songs of pleasant glee , On a cloud I saw a child , And he laughing said to me : `` Pipe a song about a Lamb ! '' So I piped with merry cheer . `` Piper , pipe that song again ; '' So I piped : he wept to hear . `` Drop thy pipe , thy happy pipe ; Sing thy songs of happy cheer : ! '' So I sang the same again , While he wept with joy to hear . `` Piper , sit thee down and write In a book , that all may read . '' So he vanish 'd from my sight ; And I pluck 'd a hollow reed , And I made a rural pen , And I stain 'd the water clear , And I wrote my happy songs Every child may joy to hear . THE SHEPHERD How sweet is the Shepherd 's sweet lot ! From the morn to the evening he stays ; He shall follow his sheep all the day , And his tongue shall be filled with praise . For he hears the lambs ' innocent call , And he hears the ewes ' tender reply ; He is watching while they are in peace , For they know when their Shepherd is nigh . THE ECHOING GREEN The sun does arise , And make happy the skies ; The merry bells ring To welcome the Spring ; The skylark and thrush , The birds of the bush , Sing louder around To the bells ' cheerful sound ; While our sports shall be seen On the echoing Green . Old John , with white hair , Does laugh away care , Sitting under the oak , Among the old folk . They laugh at our play , And soon they all say , `` Such , such were the joys When we all -- girls and boys -- In our youth-time were seen On the echoing Green . '' Till the little ones , weary , No more can be merry : The sun does descend , And our sports have an end . Round the laps of their mothers Many sisters and brothers , Like birds in their nest , Are ready for rest , And sport no more seen On the darkening green . THE LAMB Little Lamb , who make thee Dost thou know who made thee , Gave thee life , and bid thee feed By the stream and o'er the mead ; Gave thee clothing of delight , Softest clothing , wolly , bright ; Gave thee such a tender voice , Making all the vales rejoice ? Little Lamb , who made thee ? Dost thou know who made thee ? Little Lamb , I 'll tell thee ; Little Lamb , I 'll tell thee : He is called by thy name , For He calls Himself a Lamb He is meek , and He is mild , He became a little child . I a child , and thou a lamb , We are called by His name . Little Lamb , God bless thee ! Little Lamb , God bless thee ! THE LITTLE BLACK BOY My mother bore me in the southern wild , And I am black , but oh my soul is white ! White as an angel is the English child , But I am black , as if bereaved of light . My mother taught me underneath a tree , And , sitting down before the heat of day , She took me on her lap and kissed me , And , pointed to the east , began to say : `` Look on the rising sun : there God does live , And gives His light , and gives His heat away , And flowers and trees and beasts and men receive Comfort in morning , joy in the noonday . `` And we are put on earth a little space , That we may learn to bear the beams of love And these black bodies and this sunburnt face Is but a cloud , and like a shady grove . `` For when our souls have learn 'd the heat to bear , The cloud will vanish , we shall hear His voice , Saying , 'Come out from the grove , my love and care And round my golden tent like lambs rejoice ' , '' Thus did my mother say , and kissed me ; And thus I say to little English boy . When I from black and he from white cloud free , And round the tent of God like lambs we joy I 'll shade him from the heat till he can bear To lean in joy upon our Father 's knee ; And then I 'll stand and stroke his silver hair , And be like him , and he will then love me . THE BLOSSOM Merry , merry sparrow ! Under leaves so green A happy blossom Sees you , swift as arrow , Seek your cradle narrow , Near my bosom . Pretty , pretty robin ! Under leaves so green A happy blossom Hears you sobbing , sobbing , Pretty , pretty robin , Near my bosom . THE CHIMNEY-SWEEPER When my mother died I was very young , And my father sold me while yet my tongue Could scarcely cry `` Weep ! weep ! weep ! weep ! '' So your chimneys I sweep , and in soot I sleep . There 's little Tom Dacre , who cried when his head , That curled like a lamb 's back , was shaved ; so I said , `` Hush , Tom ! never mind it , for , when your head 's bare , You know that the soot can not spoil your white hair . '' And so he was quiet , and that very night , As Tom was a-sleeping , he had such a sight ! -- That thousands of sweepers , Dick , Joe , Ned , and Jack , Were all of them locked up in coffins of black . And by came an angel , who had a bright key , And he opened the coffins , and let them all free ; Then down a green plain , leaping , laughing , they run , And wash in a river , and shine in the sun . Then naked and white , all their bags left behind , They rise upon clouds , and sport in the wind ; And the Angel told Tom , if he 'd be a good boy , He 'd have God for his father , and never want joy . And so Tom awoke , and we rose in the dark , And got with our bags and our brushes to work . Though the morning was cold , Tom was happy and warm : So , if all do their duty , they need not fear harm . THE LITTLE BOY LOST `` Father , father , where are you going ? Oh do not walk so fast ! Speak , father , speak to you little boy , Or else I shall be lost . '' The night was dark , no father was there , The child was wet with dew ; The mire was deep , and the child did weep , And away the vapour flew . THE LITTLE BOY FOUND The little boy lost in the lonely fen , Led by the wandering light , Began to cry , but God , ever nigh , Appeared like his father , in white . He kissed the child , and by the hand led , And to his mother brought , Who in sorrow pale , through the lonely dale , The little boy weeping sought . LAUGHING SONG When the green woods laugh with the voice of joy , And the dimpling stream runs laughing by ; When the air does laugh with our merry wit , And the green hill laughs with the noise of it ; when the meadows laugh with lively green , And the grasshopper laughs in the merry scene , When Mary and Susan and Emily With their sweet round mouths sing `` Ha , ha he ! '' When the painted birds laugh in the shade , Where our table with cherries and nuts is spread : Come live , and be merry , and join with me , To sing the sweet chorus of `` Ha , ha , he ! '' A SONG Sweet dreams , form a shade O'er my lovely infant 's head ! Sweet dreams of pleasant streams By happy , silent , moony beams ! Sweet Sleep , with soft down Weave thy brows an infant crown Sweet Sleep , angel mild , Hover o'er my happy child ! Sweet smiles , in the night Hover over my delight ! Sweet smiles , mother 's smile , All the livelong night beguile . Sweet moans , dovelike sighs , Chase not slumber from thine eyes ! Sweet moan , sweeter smile , All the dovelike moans beguile . Sleep , sleep , happy child ! All creation slept and smiled . Sleep , sleep , happy sleep , While o'er thee doth mother weep . Sweet babe , in thy face Holy image I can trace ; Sweet babe , once like thee Thy Maker lay , and wept for me : Wept for me , for thee , for all , When He was an infant small . Thou His image ever see , Heavenly face that smiles on thee ! Smiles on thee , on me , on all , Who became an infant small ; Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE IMAGE To Mercy , Pity , Peace , and Love , All pray in their distress , And to these virtues of delight Return their thankfulness . For Mercy , Pity , Peace , and Love , Is God our Father dear ; And Mercy , Pity , Peace , and Love , Is man , his child and care . For Mercy has a human heart Pity , a human face ; And Love , the human form divine ; And Peace , the human dress . Then every man , of every clime , That prays in his distress , Prays to the human form divine : Love , Mercy , Pity , Peace . And all must love the human form , In heathen , Turk , or Jew . Where Mercy , Love , and Pity dwell , There God is dwelling too . HOLY THURSDAY 'T was on a Holy Thursday , their innocent faces clean , Came children walking two and two , in read , and blue , and green : Grey-headed beadles walked before , with wands as white as snow , Till into the high dome of Paul 's they like Thames waters flow . Oh what a multitude they seemed , these flowers of London town ! Seated in companies they sit , with radiance all their own . The hum of multitudes was there , but multitudes of lambs , Thousands of little boys and girls raising their innocent hands . Now like a mighty wild they raise to heaven the voice of song , Or like harmonious thunderings the seats of heaven among : Beneath them sit the aged man , wise guardians of the poor . Then cherish pity , lest you drive an angel from\", 'blake-poems.txt'), (\"like Thames waters flow . Oh what a multitude they seemed , these flowers of London town ! Seated in companies they sit , with radiance all their own . The hum of multitudes was there , but multitudes of lambs , Thousands of little boys and girls raising their innocent hands . Now like a mighty wild they raise to heaven the voice of song , Or like harmonious thunderings the seats of heaven among : Beneath them sit the aged man , wise guardians of the poor . Then cherish pity , lest you drive an angel from your door . NIGHT The sun descending in the west , The evening star does shine ; The birds are silent in their nest , And I must seek for mine . The moon , like a flower In heaven 's high bower , With silent delight , Sits and smiles on the night . Farewell , green fields and happy grove , Where flocks have ta'en delight . Where lambs have nibbled , silent move The feet of angels bright ; Unseen they pour blessing , And joy without ceasing , On each bud and blossom , And each sleeping bosom . They look in every thoughtless nest Where birds are covered warm ; They visit caves of every beast , To keep them all from harm : If they see any weeping That should have been sleeping , They pour sleep on their head , And sit down by their bed . When wolves and tigers howl for prey , They pitying stand and weep ; Seeking to drive their thirst away , And keep them from the sheep . But , if they rush dreadful , The angels , most heedful , Receive each mild spirit , New worlds to inherit . And there the lion 's ruddy eyes Shall flow with tears of gold : And pitying the tender cries , And walking round the fold : Saying : `` Wrath by His meekness , And , by His health , sickness , Are driven away From our immortal day . `` And now beside thee , bleating lamb , I can lie down and sleep , Or think on Him who bore thy name , Graze after thee , and weep . For , washed in life 's river , My bright mane for ever Shall shine like the gold , As I guard o'er the fold . '' SPRING Sound the flute ! Now it 's mute ! Bird 's delight , Day and night , Nightingale , In the dale , Lark in sky , -- Merrily , Merrily merrily , to welcome in the year . Little boy , Full of joy ; Little girl , Sweet and small ; Cock does crow , So do you ; Merry voice , Infant noise ; Merrily , merrily , to welcome in the year . Little lamb , Here I am ; Come and lick My white neck ; Let me pull Your soft wool ; Let me kiss Your soft face ; Merrily , merrily , to welcome in the year . NURSE 'S SONG When the voices of children are heard on the green , And laughing is heard on the hill , My heart is at rest within my breast , And everything else is still . `` Then come home , my children , the sun is gone down , And the dews of night arise ; Come , come , leave off play , and let us away , Till the morning appears in the skies . '' `` No , no , let us play , for it is yet day , And we can not go to sleep ; Besides , in the sky the little birds fly , And the hills are all covered with sheep . '' `` Well , well , go and play till the light fades away , And then go home to bed . '' The little ones leaped , and shouted , and laughed , And all the hills echoed . INFANT JOY `` I have no name ; I am but two days old . '' What shall I call thee ? `` I happy am , Joy is my name . '' Sweet joy befall thee ! Pretty joy ! Sweet joy , but two days old . Sweet Joy I call thee : Thou dost smile , I sing the while ; Sweet joy befall thee ! A DREAM Once a dream did weave a shade O'er my angel-guarded bed , That an emmet lost its way Where on grass methought I lay . Troubled , wildered , and forlorn , Dark , benighted , travel-worn , Over many a tangle spray , All heart-broke , I heard her say : `` Oh my children ! do they cry , Do they hear their father sigh ? Now they look abroad to see , Now return and weep for me . '' Pitying , I dropped a tear : But I saw a glow-worm near , Who replied , `` What wailing wight Calls the watchman of the night ? `` I am set to light the ground , While the beetle goes his round : Follow now the beetle 's hum ; Little wanderer , hie thee home ! '' ON ANOTHER 'S SORROW Can I see another 's woe , And not be in sorrow too ? Can I see another 's grief , And not seek for kind relief ? Can I see a falling tear , And not feel my sorrow 's share ? Can a father see his child Weep , nor be with sorrow filled ? Can a mother sit and hear An infant groan , an infant fear ? No , no ! never can it be ! Never , never can it be ! And can He who smiles on all Hear the wren with sorrows small , Hear the small bird 's grief and care , Hear the woes that infants bear -- And not sit beside the next , Pouring pity in their breast , And not sit the cradle near , Weeping tear on infant 's tear ? And not sit both night and day , Wiping all our tears away ? Oh no ! never can it be ! Never , never can it be ! He doth give his joy to all : He becomes an infant small , He becomes a man of woe , He doth feel the sorrow too . Think not thou canst sigh a sigh , And thy Maker is not by : Think not thou canst weep a tear , And thy Maker is not year . Oh He gives to us his joy , That our grief He may destroy : Till our grief is fled an gone He doth sit by us and moan . SONGS OF EXPERIENCE INTRODUCTION Hear the voice of the Bard , Who present , past , and future , sees ; Whose ears have heard The Holy Word That walked among the ancient tree ; Calling the lapsed soul , And weeping in the evening dew ; That might control The starry pole , And fallen , fallen light renew ! `` O Earth , O Earth , return ! Arise from out the dewy grass ! Night is worn , And the morn Rises from the slumbrous mass . `` Turn away no more ; Why wilt thou turn away ? The starry floor , The watery shore , Are given thee till the break of day . '' EARTH 'S ANSWER Earth raised up her head From the darkness dread and drear , Her light fled , Stony , dread , And her locks covered with grey despair . `` Prisoned on watery shore , Starry jealousy does keep my den Cold and hoar ; Weeping o 're , I hear the father of the ancient men . `` Selfish father of men ! Cruel , jealous , selfish fear ! Can delight , Chained in night , The virgins of youth and morning bear ? `` Does spring hide its joy , When buds and blossoms grow ? Does the sower Sow by night , Or the plowman in darkness plough ? `` Break this heavy chain , That does freeze my bones around ! Selfish , vain , Eternal bane , That free love with bondage bound . '' THE CLOD AND THE PEBBLE `` Love seeketh not itself to please , Nor for itself hath any care , But for another gives it ease , And builds a heaven in hell 's despair . '' So sang a little clod of clay , Trodden with the cattle 's feet , But a pebble of the brook Warbled out these metres meet : `` Love seeketh only Self to please , To bind another to its delight , Joys in another 's loss of ease , And builds a hell in heaven 's despite . '' HOLY THURSDAY Is this a holy thing to see In a rich and fruitful land , -- Babes reduced to misery , Fed with cold and usurous hand ? Is that trembling cry a song ? Can it be a song of joy ? And so many children poor ? It is a land of poverty ! And their son does never shine , And their fields are bleak and bare , And their ways are filled with thorns : It is eternal winter there . For where'er the sun does shine , And where'er the rain does fall , Babes should never hunger there , Nor poverty the mind appall . THE LITTLE GIRL LOST In futurity I prophetic see That the earth from sleep ( Grave the sentence deep ) Shall arise , and seek for her Maker meek ; And the desert wild Become a garden mild . In the southern clime , Where the summer 's prime Never fades away , Lovely Lyca lay . Seven summers old Lovely Lyca told . She had wandered long , Hearing wild birds ' song . `` Sweet sleep , come to me Underneath this tree ; Do father , mother , weep ? Where can Lyca sleep ? `` Lost in desert wild Is your little child . How can Lyca sleep If her mother weep ? `` If her heart does ache , Then let Lyca wake ; If my mother sleep , Lyca shall not weep . `` Frowning , frowning night , O'er this desert bright Let thy moon arise , While I close my eyes . '' Sleeping Lyca lay While the beasts of prey , Come from caverns deep , Viewed the maid asleep . The kingly lion stood , And the virgin viewed : Then he gambolled round O'er the hallowed ground . Leopards , tigers , play Round her as she lay ; While the lion old Bowed his mane of gold , And her breast did lick And upon her neck , From his eyes of flame , Ruby tears there came ; While the lioness Loosed her slender dress , And naked they conveyed To caves the sleeping maid . THE LITTLE GIRL FOUND All the night in woe Lyca 's parents go Over valleys deep , While the deserts weep . Tired and woe-begone , Hoarse with making moan , Arm in arm , seven days They traced the desert ways . Seven nights they sleep Among shadows deep , And dream they see their child Starved in desert wild . Pale through pathless ways The fancied image strays , Famished , weeping , weak , With hollow piteous shriek . Rising from unrest , The trembling woman presse With feet of weary woe ; She could no\", 'blake-poems.txt')]\n",
      "Extracted 15 samples from 2 files.\n"
     ]
    }
   ],
   "source": [
    "file_ids = ['blake-poems.txt', 'shakespeare-macbeth.txt']\n",
    "samples = get_samples(file_ids, chunk_size=2000, overlap=100)\n",
    "print(f\"Samples: {samples[:2]}\")\n",
    "print(f\"Extracted {len(samples)} samples from {len(file_ids)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6b920",
   "metadata": {},
   "source": [
    "## Style Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1b3a2e",
   "metadata": {},
   "source": [
    "### Character Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993abe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_character_features(text):\n",
    "    \"\"\"Extract character-level features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Average word length\n",
    "    words = word_tokenize(text.lower())\n",
    "    if words:\n",
    "        features['avg_word_length'] = sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        features['avg_word_length'] = 0\n",
    "        \n",
    "    # Punctuation frequency\n",
    "    punct_count = sum(1 for char in text if char in \".,;:!?-\\\"'()[]{}\")\n",
    "    features['punct_freq'] = punct_count / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Uppercase frequency\n",
    "    uppercase_count = sum(1 for char in text if char.isupper())\n",
    "    features['uppercase_freq'] = uppercase_count / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Digit frequency\n",
    "    digit_count = sum(1 for char in text if char.isdigit())\n",
    "    features['digit_freq'] = digit_count / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Character n-grams (bigrams and trigrams)\n",
    "    char_bigrams = [text[i:i+2] for i in range(len(text)-1)]\n",
    "    char_trigrams = [text[i:i+3] for i in range(len(text)-2)]\n",
    "    \n",
    "    # Top 5 character bigrams and trigrams\n",
    "    if char_bigrams:\n",
    "        bigram_counts = Counter(char_bigrams)\n",
    "        for i, (bigram, _) in enumerate(bigram_counts.most_common(5)):\n",
    "            features[f'top_char_bigram_{i+1}'] = bigram\n",
    "            \n",
    "    if char_trigrams:\n",
    "        trigram_counts = Counter(char_trigrams)\n",
    "        for i, (trigram, _) in enumerate(trigram_counts.most_common(5)):\n",
    "            features[f'top_char_trigram_{i+1}'] = trigram\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9363d",
   "metadata": {},
   "source": [
    "#### Test extract_character_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "441e00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [{'avg_word_length': 3.4495, 'punct_freq': 0.04472915261856597, 'uppercase_freq': 0.06316026073274893, 'digit_freq': 0.0004495392222971454, 'top_char_bigram_1': 'e ', 'top_char_bigram_2': 'he', 'top_char_bigram_3': ' ,', 'top_char_bigram_4': ', ', 'top_char_bigram_5': 'd ', 'top_char_trigram_1': ' , ', 'top_char_trigram_2': ' th', 'top_char_trigram_3': 'the', 'top_char_trigram_4': 'nd ', 'top_char_trigram_5': 'he '}, {'avg_word_length': 3.564, 'punct_freq': 0.04360688068368577, 'uppercase_freq': 0.055878163690150105, 'digit_freq': 0.0, 'top_char_bigram_1': 'e ', 'top_char_bigram_2': 'he', 'top_char_bigram_3': ' ,', 'top_char_bigram_4': ', ', 'top_char_bigram_5': 'th', 'top_char_trigram_1': ' , ', 'top_char_trigram_2': ' th', 'top_char_trigram_3': 'the', 'top_char_trigram_4': 'he ', 'top_char_trigram_5': 'nd '}]\n",
      "Extracted features for 15 samples.\n"
     ]
    }
   ],
   "source": [
    "file_ids = ['blake-poems.txt', 'shakespeare-macbeth.txt']\n",
    "samples = get_samples(file_ids, chunk_size=2000, overlap=100)\n",
    "texts, authors = zip(*samples)\n",
    "features = [extract_character_features(text) for text in texts]\n",
    "print(f\"Features: {features[:2]}\")\n",
    "print(f\"Extracted features for {len(features)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b18c0",
   "metadata": {},
   "source": [
    "### Word Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c229473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_features(text):\n",
    "    \"\"\"Extract word-level features\"\"\"\n",
    "    features = {}\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    if not words:\n",
    "        return {'vocab_richness': 0, 'avg_sentence_length': 0}\n",
    "    \n",
    "    # Vocabulary richness (type-token ratio)\n",
    "    features['vocab_richness'] = len(set(words)) / len(words)\n",
    "    \n",
    "    # Average sentence length\n",
    "    sentences = sent_tokenize(text)\n",
    "    if sentences:\n",
    "        words_per_sentence = [len(word_tokenize(s)) for s in sentences]\n",
    "        features['avg_sentence_length'] = sum(words_per_sentence) / len(sentences)\n",
    "    else:\n",
    "        features['avg_sentence_length'] = 0\n",
    "        \n",
    "    # Function word usage (common words like 'the', 'and', 'of', etc.)\n",
    "    function_words = ['the', 'and', 'of', 'to', 'a', 'in', 'that', 'is', 'was', 'for']\n",
    "    for word in function_words:\n",
    "        features[f'freq_{word}'] = words.count(word) / len(words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "# extract_word_features(\"This is a test string. It contains some punctuation, numbers 123, and uppercase letters. The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2b886",
   "metadata": {},
   "source": [
    "#### Test extract_word_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61548aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [{'vocab_richness': 0.2765, 'avg_sentence_length': 25.641025641025642, 'freq_the': 0.047, 'freq_and': 0.0465, 'freq_of': 0.015, 'freq_to': 0.0115, 'freq_a': 0.0155, 'freq_in': 0.013, 'freq_that': 0.0045, 'freq_is': 0.0065, 'freq_was': 0.0065, 'freq_for': 0.0065}, {'vocab_richness': 0.3085, 'avg_sentence_length': 20.61855670103093, 'freq_the': 0.0475, 'freq_and': 0.0375, 'freq_of': 0.015, 'freq_to': 0.0105, 'freq_a': 0.0125, 'freq_in': 0.014, 'freq_that': 0.005, 'freq_is': 0.0075, 'freq_was': 0.0005, 'freq_for': 0.0055}]\n",
      "Extracted features for 15 samples.\n"
     ]
    }
   ],
   "source": [
    "file_ids = ['blake-poems.txt', 'shakespeare-macbeth.txt']\n",
    "samples = get_samples(file_ids, chunk_size=2000, overlap=100)\n",
    "texts, authors = zip(*samples)\n",
    "features = [extract_word_features(text) for text in texts]\n",
    "print(f\"Features: {features[:2]}\")\n",
    "print(f\"Extracted features for {len(features)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7c7ea",
   "metadata": {},
   "source": [
    "### Syntax Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1da71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_syntax_features(text):\n",
    "    \"\"\"Extract syntax-level features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # POS tag frequencies\n",
    "    words = word_tokenize(text)\n",
    "    if not words:\n",
    "        return {'noun_freq': 0, 'verb_freq': 0, 'adj_freq': 0, 'adv_freq': 0}\n",
    "        \n",
    "    pos_tags = pos_tag(words)\n",
    "    pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "    \n",
    "    # Calculate frequencies of main POS categories\n",
    "    total_tags = len(pos_tags)\n",
    "    features['noun_freq'] = sum(pos_counts[tag] for tag in pos_counts if tag.startswith('NN')) / total_tags\n",
    "    features['verb_freq'] = sum(pos_counts[tag] for tag in pos_counts if tag.startswith('VB')) / total_tags\n",
    "    features['adj_freq'] = sum(pos_counts[tag] for tag in pos_counts if tag.startswith('JJ')) / total_tags\n",
    "    features['adv_freq'] = sum(pos_counts[tag] for tag in pos_counts if tag.startswith('RB')) / total_tags\n",
    "    \n",
    "    return features\n",
    "\n",
    "# extract_syntax_features(\"This is a test string. It contains some punctuation, numbers 123, and uppercase letters. The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c33c9",
   "metadata": {},
   "source": [
    "#### Test extract_syntax_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ae3bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [{'noun_freq': 0.268, 'verb_freq': 0.1175, 'adj_freq': 0.0675, 'adv_freq': 0.029}, {'noun_freq': 0.281, 'verb_freq': 0.1385, 'adj_freq': 0.046, 'adv_freq': 0.0365}]\n",
      "Extracted features for 15 samples.\n"
     ]
    }
   ],
   "source": [
    "file_ids = ['blake-poems.txt', 'shakespeare-macbeth.txt']\n",
    "samples = get_samples(file_ids, chunk_size=2000, overlap=100)\n",
    "texts, authors = zip(*samples)\n",
    "features = [extract_syntax_features(text) for text in texts]\n",
    "print(f\"Features: {features[:2]}\")\n",
    "print(f\"Extracted features for {len(features)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94adeabf",
   "metadata": {},
   "source": [
    "### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(text):\n",
    "    \"\"\"Extract all stylometric features from text\"\"\"\n",
    "    character_features = extract_character_features(text)\n",
    "    word_features = extract_word_features(text)\n",
    "    syntax_features = extract_syntax_features(text)\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = {**character_features, **word_features, **syntax_features}\n",
    "        \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285bf103",
   "metadata": {},
   "source": [
    "#### Test extract_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d577203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [{'avg_word_length': 3.4495, 'punct_freq': 0.04472915261856597, 'uppercase_freq': 0.06316026073274893, 'digit_freq': 0.0004495392222971454, 'top_char_bigram_1': 'e ', 'top_char_bigram_2': 'he', 'top_char_bigram_3': ' ,', 'top_char_bigram_4': ', ', 'top_char_bigram_5': 'd ', 'top_char_trigram_1': ' , ', 'top_char_trigram_2': ' th', 'top_char_trigram_3': 'the', 'top_char_trigram_4': 'nd ', 'top_char_trigram_5': 'he ', 'vocab_richness': 0.2765, 'avg_sentence_length': 25.641025641025642, 'freq_the': 0.047, 'freq_and': 0.0465, 'freq_of': 0.015, 'freq_to': 0.0115, 'freq_a': 0.0155, 'freq_in': 0.013, 'freq_that': 0.0045, 'freq_is': 0.0065, 'freq_was': 0.0065, 'freq_for': 0.0065, 'noun_freq': 0.268, 'verb_freq': 0.1175, 'adj_freq': 0.0675, 'adv_freq': 0.029}, {'avg_word_length': 3.564, 'punct_freq': 0.04360688068368577, 'uppercase_freq': 0.055878163690150105, 'digit_freq': 0.0, 'top_char_bigram_1': 'e ', 'top_char_bigram_2': 'he', 'top_char_bigram_3': ' ,', 'top_char_bigram_4': ', ', 'top_char_bigram_5': 'th', 'top_char_trigram_1': ' , ', 'top_char_trigram_2': ' th', 'top_char_trigram_3': 'the', 'top_char_trigram_4': 'he ', 'top_char_trigram_5': 'nd ', 'vocab_richness': 0.3085, 'avg_sentence_length': 20.61855670103093, 'freq_the': 0.0475, 'freq_and': 0.0375, 'freq_of': 0.015, 'freq_to': 0.0105, 'freq_a': 0.0125, 'freq_in': 0.014, 'freq_that': 0.005, 'freq_is': 0.0075, 'freq_was': 0.0005, 'freq_for': 0.0055, 'noun_freq': 0.281, 'verb_freq': 0.1385, 'adj_freq': 0.046, 'adv_freq': 0.0365}]\n",
      "Extracted features for 15 samples.\n"
     ]
    }
   ],
   "source": [
    "file_ids = ['blake-poems.txt', 'shakespeare-macbeth.txt']\n",
    "samples = get_samples(file_ids, chunk_size=2000, overlap=100)\n",
    "texts, authors = zip(*samples)\n",
    "features = [extract_all_features(text) for text in texts]\n",
    "print(f\"Features: {features[:2]}\")\n",
    "print(f\"Extracted features for {len(features)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01670b67",
   "metadata": {},
   "source": [
    "## Transformer Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fb90e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(samples, max_length=512):\n",
    "    \"\"\"\n",
    "    Extract embeddings from transformer model.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of text samples\n",
    "        max_length: Maximum token length for the model\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    model_name='bert-base-uncased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        # Tokenize and prepare input\n",
    "        inputs = tokenizer(sample, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # Use the [CLS] token embedding (first token) as the text representation\n",
    "        # Shape: [batch_size, hidden_size]\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(embedding.flatten())\n",
    "        \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bb8e0",
   "metadata": {},
   "source": [
    "#### Test extract_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2fbcac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape:  (761, 1536)\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(['blake-poems.txt', 'shakespeare-macbeth.txt'], chunk_size=50, overlap=10) # 7min\n",
    "embeddings = extract_embeddings(samples)\n",
    "print(\"Embeddings shape: \", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b420ef",
   "metadata": {},
   "source": [
    "## Author Attributor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2422a7",
   "metadata": {},
   "source": [
    "### Feature Based\n",
    "Author attribution pipeline using feature-based stylometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2651c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(samples):\n",
    "    \"\"\"\n",
    "    Prepare data for training.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of (text, author) tuples\n",
    "        \n",
    "    Returns:\n",
    "        feature_matrix: Feature matrix\n",
    "        labels: Target labels\n",
    "        feature_names: List of feature names\n",
    "        author_to_idx: Mapping from author names to indices\n",
    "    \"\"\"\n",
    "    texts, authors = zip(*samples)\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_authors = sorted(set(authors))\n",
    "    author_to_idx = {author: i for i, author in enumerate(unique_authors)}\n",
    "    labels = np.array([author_to_idx[author] for author in authors])\n",
    "    \n",
    "    # Extract features\n",
    "    features = [extract_all_features(text) for text in texts]\n",
    "    feature_names = list(features[0].keys())\n",
    "    # feature_matrix = np.array([[feat[name] for name in feature_names] for feat in features])\n",
    "    feature_matrix = np.array([[float(feat.get(name, 0)) if str(feat.get(name, 0)).replace('.', '', 1).isdigit() else 0 for name in feature_names] for feat in features])\n",
    "\n",
    "    # return feature_matrix, labels, author_to_idx\n",
    "    return feature_matrix, labels, feature_names, author_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184b78f",
   "metadata": {},
   "source": [
    "#### Test prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64e68017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (761, 30)\n",
      "Labels shape: (761,)\n",
      "Feature names: ['avg_word_length', 'punct_freq', 'uppercase_freq', 'digit_freq', 'top_char_bigram_1']\n",
      "Author to index mapping: {'blake-poems.txt': 0, 'shakespeare-macbeth.txt': 1}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(['blake-poems.txt', 'shakespeare-macbeth.txt'], chunk_size=50, overlap=10)\n",
    "feature_matrix, labels, feature_names, author_to_idx = prepare_data(samples)\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Feature names: {feature_names[:5]}\")\n",
    "print(f\"Author to index mapping: {author_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d052542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(feature_matrix, labels):\n",
    "    \"\"\"\n",
    "    Train the author attribution model.\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix: Feature matrix\n",
    "        labels: Target labels\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Try both SVM and Random Forest\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='rbf', probability=True))\n",
    "    ])\n",
    "    \n",
    "    rf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Find best hyperparameters for SVM\n",
    "    svm_param_grid = {\n",
    "        'svm__C': [0.1, 1, 10, 100],\n",
    "        'svm__gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "    }\n",
    "    svm_grid = GridSearchCV(svm_pipeline, svm_param_grid, cv=5, scoring='f1_macro')\n",
    "    svm_grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Find best hyperparameters for Random Forest\n",
    "    rf_param_grid = {\n",
    "        'rf__n_estimators': [50, 100],\n",
    "        'rf__max_depth': [None, 10, 20]\n",
    "    }\n",
    "    rf_grid = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='f1_macro')\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Compare both models\n",
    "    print(f\"SVM best score: {svm_grid.best_score_}\")\n",
    "    print(f\"Random Forest best score: {rf_grid.best_score_}\")\n",
    "    \n",
    "    # Select the better model\n",
    "    if svm_grid.best_score_ >= rf_grid.best_score_:\n",
    "        model = svm_grid.best_estimator_\n",
    "        print(f\"Selected SVM with params: {svm_grid.best_params_}\")\n",
    "    else:\n",
    "        model = rf_grid.best_estimator_\n",
    "        print(f\"Selected Random Forest with params: {rf_grid.best_params_}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(\"\\nValidation Set Performance:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed616c3",
   "metadata": {},
   "source": [
    "#### Test train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f010e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best score: 0.8300702693644911\n",
      "Random Forest best score: 0.8273241673068202\n",
      "Selected SVM with params: {'svm__C': 10, 'svm__gamma': 0.01}\n",
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.64      0.71        39\n",
      "           1       0.89      0.95      0.92       114\n",
      "\n",
      "    accuracy                           0.87       153\n",
      "   macro avg       0.85      0.79      0.81       153\n",
      "weighted avg       0.87      0.87      0.86       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(['blake-poems.txt', 'shakespeare-macbeth.txt'], chunk_size=50, overlap=10)\n",
    "feature_matrix, labels, feature_names, author_to_idx = prepare_data(samples)\n",
    "model = train(feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "881e2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, feature_matrix):\n",
    "    \"\"\"\n",
    "    Predict authors for texts.\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix: Feature matrix for texts\n",
    "        \n",
    "    Returns:\n",
    "        Predicted author indices\n",
    "    \"\"\"\n",
    "\n",
    "    return model.predict(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfb386",
   "metadata": {},
   "source": [
    "#### Test predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99b2741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best score: 0.8300702693644911\n",
      "Random Forest best score: 0.8273241673068202\n",
      "Selected SVM with params: {'svm__C': 10, 'svm__gamma': 0.01}\n",
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.64      0.71        39\n",
      "           1       0.89      0.95      0.92       114\n",
      "\n",
      "    accuracy                           0.87       153\n",
      "   macro avg       0.85      0.79      0.81       153\n",
      "weighted avg       0.87      0.87      0.86       153\n",
      "\n",
      "Predictions: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(['blake-poems.txt', 'shakespeare-macbeth.txt'], chunk_size=50, overlap=10)\n",
    "feature_matrix, labels, feature_names, author_to_idx = prepare_data(samples)\n",
    "model = train(feature_matrix, labels)\n",
    "predictions = predict(model, feature_matrix)\n",
    "print('Predictions:', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d87d54",
   "metadata": {},
   "source": [
    "### Transformer Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7108da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(samples):\n",
    "    \"\"\"\n",
    "    Prepare data for training.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of (text, author) tuples\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: Embeddings from transformer model\n",
    "        labels: Target labels\n",
    "        feature_names: List of feature names\n",
    "        author_to_idx: Mapping from author names to indices\n",
    "    \"\"\"\n",
    "    texts, authors = zip(*samples)\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_authors = sorted(set(authors))\n",
    "    author_to_idx = {author: i for i, author in enumerate(unique_authors)}\n",
    "    labels = np.array([author_to_idx[author] for author in authors])\n",
    "    # Convert labels to torch tensors\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    embeddings = extract_embeddings(texts)\n",
    "    \n",
    "    return embeddings, labels, author_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f4794",
   "metadata": {},
   "source": [
    "#### Test prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3077aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (761, 768)\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(['blake-poems.txt', 'shakespeare-macbeth.txt'], chunk_size=50, overlap=10)\n",
    "embeddings, labels, author_to_idx = prepare_data(samples)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(embeddings, labels):\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f7cc7",
   "metadata": {},
   "source": [
    "#### Test train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22123e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(['blake-poems.txt', 'shakespeare-macbeth.txt'], chunk_size=50, overlap=10)\n",
    "embeddings, labels, author_to_idx = prepare_data(samples)\n",
    "model = train(embeddings, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
